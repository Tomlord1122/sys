diff --git a/include/linux/sched.h b/include/linux/sched.h
index c1a927dde..68342e701 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -580,6 +580,24 @@ struct sched_rt_entity {
 #endif
 } __randomize_layout;
 
+struct sched_mlq_entity {
+	struct list_head		run_list;
+	unsigned long			timeout;
+	unsigned long			watchdog_stamp;
+	unsigned int			time_slice;
+	unsigned short			on_rq;
+	unsigned short			on_list;
+
+	struct sched_mlq_entity		*back;
+#ifdef CONFIG_RT_GROUP_SCHED
+	struct sched_mlq_entity		*parent;
+	/* rq on which this entity is (to be) queued: */
+	struct mlq_rq			*mlq_rq;
+	/* rq "owned" by this entity/group: */
+	struct mlq_rq			*my_q;
+#endif
+};
+
 struct sched_dl_entity {
 	struct rb_node			rb_node;
 
@@ -770,15 +788,20 @@ struct task_struct {
 #endif
 	int				on_rq;
 
-	int				prio;
+	//值越低優先級越大
+	int				prio;    //mlq:140~142
 	int				static_prio;
 	int				normal_prio;
+
+	//值越高優先級越大
 	unsigned int			rt_priority;
+	unsigned int            mlq_priority;  //mlq:0~2
 
 	const struct sched_class	*sched_class;
 	struct sched_entity		se;
 	struct sched_rt_entity		rt;
 	struct sched_dl_entity		dl;
+	struct sched_mlq_entity     mlq;
 
 #ifdef CONFIG_SCHED_CORE
 	struct rb_node			core_node;
diff --git a/include/linux/sched/prio.h b/include/linux/sched/prio.h
index ab83d85e1..45d33c066 100644
--- a/include/linux/sched/prio.h
+++ b/include/linux/sched/prio.h
@@ -5,6 +5,7 @@
 #define MAX_NICE	19
 #define MIN_NICE	-20
 #define NICE_WIDTH	(MAX_NICE - MIN_NICE + 1)
+#define MLQ_WIDTH   3
 
 /*
  * Priority of a process goes from 0..MAX_PRIO-1, valid RT
@@ -14,8 +15,11 @@
  */
 
 #define MAX_RT_PRIO		100
+#define MAX_MLQ_PRIO    143
+#define MAX_MLQ_PRIO_USE (MAX_MLQ_PRIO - MAX_RT_PRIO - NICE_WIDTH)
 
-#define MAX_PRIO		(MAX_RT_PRIO + NICE_WIDTH)
+// #define MAX_PRIO		(MAX_RT_PRIO + NICE_WIDTH)
+#define MAX_PRIO		(MAX_RT_PRIO + NICE_WIDTH + MLQ_WIDTH)
 #define DEFAULT_PRIO		(MAX_RT_PRIO + NICE_WIDTH / 2)
 
 /*
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index f21714ea3..7af2eccfe 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2010,14 +2010,16 @@ void deactivate_task(struct rq *rq, struct task_struct *p, int flags)
 	dequeue_task(rq, p, flags);
 }
 
-static inline int __normal_prio(int policy, int rt_prio, int nice)
+static inline int __normal_prio(int policy, int p_priority, int nice)
 {
 	int prio;
 
 	if (dl_policy(policy))
 		prio = MAX_DL_PRIO - 1;
 	else if (rt_policy(policy))
-		prio = MAX_RT_PRIO - 1 - rt_prio;
+		prio = MAX_RT_PRIO - 1 - p_priority;
+	else if (mlq_policy(policy))
+		prio = MAX_MLQ_PRIO - 1 - p_priority;
 	else
 		prio = NICE_TO_PRIO(nice);
 
@@ -2033,7 +2035,11 @@ static inline int __normal_prio(int policy, int rt_prio, int nice)
  */
 static inline int normal_prio(struct task_struct *p)
 {
+#ifdef CONFIG_MLQ_SCHED
+	return __normal_prio(p->policy, p->mlq_priority, PRIO_TO_NICE(p->static_prio));
+#else
 	return __normal_prio(p->policy, p->rt_priority, PRIO_TO_NICE(p->static_prio));
+#endif
 }
 
 /*
@@ -4210,6 +4216,14 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	p->rt.on_rq		= 0;
 	p->rt.on_list		= 0;
 
+#ifdef CONFIG_MLQ_SCHED
+	INIT_LIST_HEAD(&p->mlq.run_list);
+	p->mlq.timeout		= 0;
+	p->mlq.time_slice	= 0;  //在mlq.c設定
+	p->mlq.on_rq		= 0;
+	p->mlq.on_list		= 0;
+#endif
+
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	INIT_HLIST_HEAD(&p->preempt_notifiers);
 #endif
@@ -4341,6 +4355,7 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 	/*
 	 * Make sure we do not leak PI boosting priority to the child.
 	 */
+#ifdef
 	p->prio = current->normal_prio;
 
 	uclamp_fork(p);
@@ -4370,6 +4385,8 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 		return -EAGAIN;
 	else if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
+	else if (mlq_prio(p->prio))
+		p->sched_class = &mlq_sched_class;
 	else
 		p->sched_class = &fair_sched_class;
 
@@ -7412,6 +7429,7 @@ static int __sched_setscheduler(struct task_struct *p,
 	oldprio = p->prio;
 
 	newprio = __normal_prio(policy, attr->sched_priority, attr->sched_nice);
+
 	if (pi) {
 		/*
 		 * Take priority boosted tasks into account. If the new
@@ -9288,7 +9306,8 @@ void __init sched_init(void)
 
 	/* Make sure the linker didn't screw up */
 	BUG_ON(&idle_sched_class + 1 != &fair_sched_class ||
-	       &fair_sched_class + 1 != &rt_sched_class ||
+	       &fair_sched_class + 1 != &mlq_sched_class ||
+		   &mlq_sched_class + 1 != &rt_sched_class  ||
 	       &rt_sched_class + 1   != &dl_sched_class);
 #ifdef CONFIG_SMP
 	BUG_ON(&dl_sched_class + 1 != &stop_sched_class);
@@ -9365,6 +9384,8 @@ void __init sched_init(void)
 		init_cfs_rq(&rq->cfs);
 		init_rt_rq(&rq->rt);
 		init_dl_rq(&rq->dl);
+		init_mlq_rq(&rq->mlq);
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
 		rq->tmp_alone_branch = &rq->leaf_cfs_rq_list;
@@ -9461,6 +9482,10 @@ void __init sched_init(void)
 #endif
 	init_sched_fair_class();
 
+// #ifdef CONFIG_MLQ_SCHED
+//	init_sched_mlq_class();
+// #endif
+
 	psi_init();
 
 	init_uclamp();
diff --git a/kernel/sched/mlq.c b/kernel/sched/mlq.c
new file mode 100644
index 000000000..1ebe6e3c8
--- /dev/null
+++ b/kernel/sched/mlq.c
@@ -0,0 +1,744 @@
+// SPDX-License-Identifier: UNLICENSED
+#include "sched.h"
+#include "pelt.h"
+
+/* Constants for MLQ scheduler */
+#define MLQ_PRIO_MIN 140
+#define MLQ_PRIO_MAX 142
+
+/* Time slice for MLQ priorities */
+#define TIME_SLICE_P1 50
+#define TIME_SLICE_P2 100
+
+#define RUNTIME_INF	((u64)~0ULL)
+#define for_each_sched_mlq_entity(mlq_se) \
+	for (; mlq_se; mlq_se = NULL)
+
+#define mlq_entity_is_task(mlq_se) (1)
+
+void init_mlq_rq(struct mlq_rq *mlq_rq)
+{
+	struct mlq_prio_array *array;
+	int i;
+
+	array = &mlq_rq->active;
+	for (i = 0; i < MAX_MLQ_PRIO_USE; i++) {
+		INIT_LIST_HEAD(array->queue + i);
+		__clear_bit(i, array->bitmap);
+	}
+	/* delimiter for bitsearch: */
+	__set_bit(MAX_MLQ_PRIO_USE, array->bitmap);
+
+#if defined CONFIG_SMP
+	rt_rq->highest_prio.curr = MAX_MLQ_PRIO-1;
+	rt_rq->highest_prio.next = MAX_MLQ_PRIO-1;
+	rt_rq->rt_nr_migratory = 0;
+	rt_rq->overloaded = 0;
+	plist_head_init(&rt_rq->pushable_tasks);
+#endif /* CONFIG_SMP */
+	/* We start is dequeued state, because no RT tasks are queued */
+	mlq_rq->mlq_queued = 0;
+
+	mlq_rq->mlq_time = 0;
+	mlq_rq->mlq_throttled = 0;
+	mlq_rq->mlq_runtime = 0;
+	raw_spin_lock_init(&mlq_rq->mlq_runtime_lock);
+}
+
+static inline struct task_struct *mlq_task_of(struct sched_mlq_entity *mlq_se)
+{
+	return container_of(mlq_se, struct task_struct, mlq);
+}
+
+static inline struct rq *rq_of_mlq_rq(struct mlq_rq *mlq_rq)
+{
+	return container_of(mlq_rq, struct rq, mlq);
+}
+
+static inline struct rq *rq_of_mlq_se(struct sched_mlq_entity *mlq_se)
+{
+	struct task_struct *p = mlq_task_of(mlq_se);
+
+	return task_rq(p);
+}
+
+static inline struct mlq_rq *mlq_rq_of_se(struct sched_mlq_entity *mlq_se)
+{
+	struct rq *rq = rq_of_mlq_se(mlq_se);
+
+	return &rq->mlq;
+}
+
+static inline int on_mlq_rq(struct sched_mlq_entity *mlq_se)
+{
+	return mlq_se->on_rq;
+}
+
+//用不到
+static inline struct mlq_rq *group_mlq_rq(struct sched_mlq_entity *mlq_se)
+{
+	return NULL;
+}
+
+static inline int mlq_rq_throttled(struct mlq_rq *mlq_rq)
+{
+	return mlq_rq->mlq_throttled;
+}
+
+static inline u64 sched_mlq_runtime(struct mlq_rq *mlq_rq)
+{
+	return mlq_rq->mlq_runtime; /*注意 mlq_runtime 是否存在? */
+}
+
+//task_struct->prio為我們設定的per-process priority(1~3)
+static inline int mlq_se_prio(struct sched_mlq_entity *mlq_se)
+{
+#ifdef CONFIG_RT_GROUP_SCHED
+	struct mlq_rq *mlq_rq = group_mlq_rq(mlq_se);
+
+	if (mlq_rq)
+		return mlq_rq->highest_prio.curr;
+#endif
+
+	return mlq_task_of(mlq_se)->prio;
+}
+
+static inline int mlq_prio(int prio)
+{
+	if (unlikely(prio < MAX_MLQ_PRIO))
+		return 1;
+	return 0;
+}
+
+static inline
+unsigned int mlq_se_nr_running(struct sched_mlq_entity *mlq_se)
+{
+	struct mlq_rq *group_rq = group_mlq_rq(mlq_se);
+
+	if (group_rq)
+		return group_rq->mlq_nr_running;
+	else
+		return 1;
+}
+
+static inline
+unsigned int mlq_se_rr_nr_running(struct sched_mlq_entity *mlq_se)
+{
+	struct mlq_rq *group_rq = group_mlq_rq(mlq_se);
+	struct task_struct *tsk;
+
+	if (group_rq)
+		return group_rq->rr_nr_running;
+
+	tsk = mlq_task_of(mlq_se);
+
+	//有time_slice則為RR
+	if (tsk->policy == SCHED_MLQ)
+		return tsk->mlq.time_slice ? 1 : 0;
+}
+
+static inline
+void inc_mlq_tasks(struct sched_mlq_entity *mlq_se, struct mlq_rq *mlq_rq)
+{
+	int prio = mlq_se_prio(mlq_se);
+
+	WARN_ON(!mlq_prio(prio));
+	mlq_rq->mlq_nr_running += mlq_se_nr_running(mlq_se);
+	mlq_rq->mlq_nr_running += mlq_se_rr_nr_running(mlq_se);
+
+	// 只有group scheduling要做
+	// inc_rt_prio(rt_rq, prio);
+	// inc_rt_migration(rt_se, rt_rq);
+	// inc_rt_group(rt_se, rt_rq);
+}
+
+static inline
+void dec_mlq_tasks(struct sched_mlq_entity *mlq_se, struct mlq_rq *mlq_rq)
+{
+	WARN_ON(!mlq_prio(mlq_se_prio(mlq_se)));
+	WARN_ON(!mlq_rq->mlq_nr_running);
+	mlq_rq->mlq_nr_running -= mlq_se_nr_running(mlq_se);
+	mlq_rq->rr_nr_running -= mlq_se_rr_nr_running(mlq_se);
+
+	// 只有group scheduling要做
+	// dec_rt_prio(rt_rq, rt_se_prio(mlq_se));
+	// dec_rt_migration(mlq_se, rt_rq);
+	// dec_rt_group(mlq_se, rt_rq);
+}
+
+/*
+ * Change rt_se->run_list location unless SAVE && !MOVE
+ *
+ * assumes ENQUEUE/DEQUEUE flags match
+ */
+static inline bool move_entity(unsigned int flags)
+{
+	if ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) == DEQUEUE_SAVE)
+		return false;
+
+	return true;
+}
+
+//重要!
+static void __delist_mlq_entity(struct sched_mlq_entity *mlq_se, struct mlq_prio_array *array)
+{
+/*
+ * 從entity list(run_list)中刪掉+初始化mlq_se
+ * 此時mlq_se->runlist->prev和next皆指向自己
+ */
+	list_del_init(&mlq_se->run_list);
+
+	if (list_empty(array->queue + mlq_se_prio(mlq_se)))
+		__clear_bit(mlq_se_prio(mlq_se), array->bitmap);
+
+	mlq_se->on_list = 0;
+}
+
+/*
+ * Adding/removing a task to/from a priority array:
+ */
+
+//重要!
+static void __enqueue_mlq_entity(struct sched_mlq_entity *mlq_se, unsigned int flags)
+{
+	struct mlq_rq *mlq_rq = mlq_rq_of_se(mlq_se);
+	struct mlq_prio_array *array = &mlq_rq->active;
+	struct mlq_rq *group_rq = group_mlq_rq(mlq_se);  //應該不會用到
+	struct list_head *queue = array->queue + mlq_se_prio(mlq_se);
+
+	/*
+	 * Don't enqueue the group if its throttled, or when empty.
+	 * The latter is a consequence of the former when a child group
+	 * get throttled and the current group doesn't have any other
+	 * active members.
+	 */
+	//if (group_rq && (rt_rq_throttled(group_rq) || !group_rq->rt_nr_running)) {
+	//	if (mlq_se->on_list)
+	//		__delist_rt_entity(mlq_se, array);
+	//	return;
+	//}
+
+	if (move_entity(flags)) {
+		WARN_ON_ONCE(mlq_se->on_list);
+		if (flags & ENQUEUE_HEAD)
+			list_add(&mlq_se->run_list, queue);
+		else
+			list_add_tail(&mlq_se->run_list, queue);
+
+		__set_bit(mlq_se_prio(mlq_se), array->bitmap);
+		mlq_se->on_list = 1;
+	}
+	mlq_se->on_rq = 1;
+
+	inc_mlq_tasks(mlq_se, mlq_rq);
+}
+
+static void __dequeue_mlq_entity(struct sched_mlq_entity *mlq_se, unsigned int flags)
+{
+	struct mlq_rq *mlq_rq = mlq_rq_of_se(mlq_se);
+	struct mlq_prio_array *array = &mlq_rq->active;
+
+	if (move_entity(flags)) {
+		WARN_ON_ONCE(!mlq_se->on_list);
+		__delist_mlq_entity(mlq_se, array);
+	}
+	mlq_se->on_rq = 0;
+
+	dec_mlq_tasks(mlq_se, mlq_rq);
+}
+
+static int sched_mlq_runtime_exceeded(struct mlq_rq *mlq_rq)
+{
+	u64 runtime = sched_mlq_runtime(mlq_rq);
+
+
+	// *紀錄 這個應該用不到 因為mlq_nr_boosted是SMP的CONFIG
+	// if (mlq_rq->mlq_throttled)
+	//	return rt_rq_throttled(rt_rq);
+
+
+	/*
+	 *if (runtime >= sched_rt_period(rt_rq)) //*紀錄 如上
+	 *	return 0;
+	 */
+
+	//balance_runtime(rt_rq);
+	runtime = sched_mlq_runtime(mlq_rq);
+	if (runtime == RUNTIME_INF)
+		return 0;
+
+//
+//   先看這個 寫到這裡我發現有致命問題
+//   注意 這裡我沒改好
+//   因為這裡用到的 bandwidth 和 throttling 好像跟 task group 有關
+//   這個部分是用來讓 rt_rq 整體執行時間太長後要換成別的 rt_rq
+//   意義上就是換 task group 執行
+//   我覺得邏輯可能怪怪的 要再討論一下
+
+//	if (rt_rq->rt_time > runtime) {
+//		struct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);
+
+//		if (likely(rt_b->rt_runtime)) {
+//			rt_rq->rt_throttled = 1;
+//			printk_deferred_once("sched: RT throttling activated\n");
+//		} else {
+//			rt_rq->rt_time = 0;
+//		}
+
+//		if (rt_rq_throttled(rt_rq)) {
+//			sched_rt_rq_dequeue(rt_rq);
+//			return 1;
+//		}
+//	}
+//
+	return 0;
+}
+
+static void update_curr_mlq(struct rq *rq)
+{
+	struct task_struct *curr = rq->curr;
+	struct sched_mlq_entity *mlq_se = &curr->mlq; // *注意 task_struct 是否有新增mlq?
+	u64 delta_exec;
+	u64 now;
+
+	if (curr->sched_class != &mlq_sched_class) // *注意 mlq_sched_class 是否有新增
+		return;
+
+	now = rq_clock_task(rq);
+	delta_exec = now - curr->se.exec_start;
+	if (unlikely((s64)delta_exec <= 0))
+		return;
+
+	schedstat_set(curr->se.statistics.exec_max,
+		      max(curr->se.statistics.exec_max, delta_exec));
+
+	curr->se.sum_exec_runtime += delta_exec;
+	// account_group_exec_runtime(curr, delta_exec); *紀錄 不使用group
+
+	curr->se.exec_start = now;
+	// cgroup_account_cputime(curr, delta_exec);     *紀錄 不使用group
+
+	// if (!rt_bandwidth_enabled())
+	//	return;
+
+	for_each_sched_mlq_entity(mlq_se) {
+		struct mlq_rq *mlq_rq = mlq_rq_of_se(mlq_se);  //注意 mlq_rq 是否存在?
+
+		if (sched_mlq_runtime(mlq_rq) != RUNTIME_INF) {// 注意 mlq_runtime_lock
+			raw_spin_lock(&mlq_rq->mlq_runtime_lock);  // 是否存在
+			mlq_rq->mlq_time += delta_exec;
+			if (sched_mlq_runtime_exceeded(mlq_rq))
+				resched_curr(rq);
+			raw_spin_unlock(&mlq_rq->mlq_runtime_lock);
+		}
+	}
+
+}
+
+// per-cpu mlq_rq
+static void
+dequeue_top_mlq_rq(struct mlq_rq *mlq_rq)
+{
+	struct rq *rq = rq_of_mlq_rq(mlq_rq);
+
+	BUG_ON(&rq->mlq != mlq_rq);
+
+	if (!mlq_rq->mlq_queued)
+		return;
+
+	BUG_ON(!rq->nr_running);
+
+	sub_nr_running(rq, mlq_rq->mlq_nr_running);
+	mlq_rq->mlq_queued = 0;
+}
+
+//重要!
+static void dequeue_mlq_stack(struct sched_mlq_entity *mlq_se, unsigned int flags)
+{
+	struct sched_mlq_entity *back = NULL;
+
+	for_each_sched_mlq_entity(mlq_se) {
+		mlq_se->back = back;
+		back = mlq_se;
+	}
+
+	// 清空rq中所有mlq_rq內的runnable task
+	dequeue_top_mlq_rq(mlq_rq_of_se(back));
+
+	for (mlq_se = back; mlq_se; mlq_se = mlq_se->back) {
+		if (on_mlq_rq(mlq_se))
+			__dequeue_mlq_entity(mlq_se, flags);
+	}
+}
+
+static void
+enqueue_top_mlq_rq(struct mlq_rq *mlq_rq)
+{
+	struct rq *rq = rq_of_mlq_rq(mlq_rq);
+
+	BUG_ON(&rq->mlq != mlq_rq);
+
+	if (mlq_rq->mlq_queued)
+		return;
+
+	if (mlq_rq_throttled(mlq_rq))
+		return;
+
+	if (mlq_rq->mlq_nr_running) {
+		add_nr_running(rq, mlq_rq->mlq_nr_running);
+		mlq_rq->mlq_queued = 1;
+	}
+
+	/* Kick cpufreq (see the comment in kernel/sched/sched.h). */
+	cpufreq_update_util(rq, 0);
+}
+
+static void enqueue_mlq_entity(struct sched_mlq_entity *mlq_se, unsigned int flags)
+{
+	struct rq *rq = rq_of_mlq_se(mlq_se);
+
+	dequeue_mlq_stack(mlq_se, flags);
+	for_each_sched_mlq_entity(mlq_se)
+		__enqueue_mlq_entity(mlq_se, flags);
+	enqueue_top_mlq_rq(&rq->mlq);
+}
+
+static void dequeue_mlq_entity(struct sched_rt_entity *mlq_se, unsigned int flags)
+{
+	struct rq *rq = rq_of_mlq_se(mlq_se);
+
+	dequeue_mlq_stack(mlq_se, flags);
+
+	for_each_sched_mlq_entity(mlq_se) {
+		struct mlq_rq *mlq_rq = group_mlq_rq(mlq_se);
+
+		if (mlq_rq && mlq_rq->mlq_nr_running)
+			__enqueue_mlq_entity(mlq_se, flags);
+	}
+	enqueue_top_mlq_rq(&rq->mlq); //?
+}
+
+static void
+enqueue_task_mlq(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct sched_mlq_entity *mlq_se = &p->mlq;
+
+	if (flags & ENQUEUE_WAKEUP)
+		mlq_se->timeout = 0;
+
+	enqueue_mlq_entity(mlq_se, flags);
+
+	// SMP才需要
+	// if (!task_current(rq, p) && p->nr_cpus_allowed > 1)
+	//	enqueue_pushable_task(rq, p);
+}
+
+static void dequeue_task_mlq(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct sched_mlq_entity *mlq_se = &p->mlq;
+
+	update_curr_mlq(rq);
+	dequeue_mlq_entity(mlq_se, flags);
+
+	// SMP才需要
+	// dequeue_pushable_task(rq, p);
+}
+
+/*
+ * Put task to the head or the end of the run list without the overhead of
+ * dequeue followed by enqueue.
+ */
+
+//重要!
+static void
+requeue_mlq_entity(struct mlq_rq *mlq_rq, struct sched_mlq_entity *mlq_se, int head)
+{
+	if (on_mlq_rq(mlq_se)) {
+		struct mlq_prio_array *array = &mlq_rq->active;
+		struct list_head *queue = array->queue + mlq_se_prio(mlq_se);
+
+		if (head)
+			list_move(&mlq_se->run_list, queue);
+		else
+			list_move_tail(&mlq_se->run_list, queue);
+	}
+}
+
+static void requeue_task_mlq(struct rq *rq, struct task_struct *p, int head)
+{
+	struct sched_mlq_entity *mlq_se = &p->mlq;
+	struct mlq_rq *mlq_rq;
+
+	for_each_sched_mlq_entity(mlq_se) {
+		mlq_rq = mlq_rq_of_se(mlq_se);
+		requeue_mlq_entity(mlq_rq, mlq_se, head);
+	}
+}
+
+//看起來是在curr的priority變化時，將該task(sched_entity)搬到新的priority queue
+static void yield_task_mlq(struct rq *rq)
+{
+	requeue_task_mlq(rq, rq->curr, 0);
+}
+
+/*
+ * Preempt the current task with a newly woken task if needed:
+ */
+static void check_preempt_curr_mlq(struct rq *rq, struct task_struct *p, int flags)
+{
+
+	if (p->prio < rq->curr->prio) {
+		resched_curr(rq);
+		return;
+	}
+
+#ifdef CONFIG_SMP
+	/*
+	 * If:
+	 *
+	 * - the newly woken task is of equal priority to the current task
+	 * - the newly woken task is non-migratable while current is migratable
+	 * - current will be preempted on the next reschedule
+	 *
+	 * we should check to see if current can readily move to a different
+	 * cpu.  If so, we will reschedule to allow the push logic to try
+	 * to move current somewhere else, making room for our non-migratable
+	 * task.
+	 */
+	if (p->prio == rq->curr->prio && !test_tsk_need_resched(rq->curr))
+		check_preempt_equal_prio(rq, p);
+#endif
+}
+
+static inline void set_next_task_mlq(struct rq *rq, struct task_struct *p, bool first)
+{
+	p->se.exec_start = rq_clock_task(rq);
+
+	/* The running task is never eligible for pushing */
+	// SMP才要做
+	// dequeue_pushable_task(rq, p);
+
+	if (!first)
+		return;
+
+	/*
+	 * If prev task was rt, put_prev_task() has already updated the
+	 * utilization. We only care of the case where we start to schedule a
+	 * rt task
+	 */
+
+	//???等等回來研究
+	if (rq->curr->sched_class != &rt_sched_class)
+		update_rt_rq_load_avg(rq_clock_pelt(rq), rq, 0);
+
+	// SMP才要做
+	// rt_queue_push_tasks(rq);
+}
+
+
+//重要!
+static struct sched_mlq_entity *pick_next_mlq_entity(struct rq *rq,
+						   struct mlq_rq *mlq_rq)
+{
+	struct mlq_prio_array *array = &mlq_rq->active;
+	struct sched_mlq_entity *next = NULL;
+	struct list_head *queue;
+	int idx;
+
+	idx = sched_find_first_bit(array->bitmap);
+	BUG_ON(idx >= MAX_MLQ_PRIO_USE);
+
+	queue = array->queue + idx;
+	next = list_entry(queue->next, struct sched_mlq_entity, run_list);
+
+	return next;
+}
+
+static struct task_struct *_pick_next_task_mlq(struct rq *rq)
+{
+	struct sched_mlq_entity *mlq_se;
+	struct mlq_rq *mlq_rq  = &rq->mlq;
+
+	do {
+		mlq_se = pick_next_mlq_entity(rq, mlq_rq);
+		BUG_ON(!mlq_se);
+		mlq_rq = group_mlq_rq(mlq_se);
+	} while (mlq_rq);
+
+	return mlq_task_of(mlq_se);
+}
+
+static struct task_struct *pick_task_mlq(struct rq *rq)
+{
+	struct task_struct *p;
+
+	if (!sched_mlq_runnable(rq))
+		return NULL;
+
+	p = _pick_next_task_mlq(rq);
+
+	return p;
+}
+
+static struct task_struct *pick_next_task_mlq(struct rq *rq)
+{
+	struct task_struct *p = pick_task_mlq(rq);
+
+	if (p)
+		set_next_task_mlq(rq, p, true);
+
+	return p;
+}
+
+static void put_prev_task_mlq(struct rq *rq, struct task_struct *p)
+{
+	update_curr_mlq(rq);
+
+	//???等等研究
+	update_rt_rq_load_avg(rq_clock_pelt(rq), rq, 1);
+
+	/*
+	 * The previous task needs to be made eligible for pushing
+	 * if it is still active
+	 */
+
+	// SMP才要
+	// if (on_mlq_rq(&p->mlq) && p->nr_cpus_allowed > 1)
+	//	enqueue_pushable_task(rq, p);
+}
+
+static void switched_to_mlq(struct rq *rq, struct task_struct *p)
+{
+	/*
+	 * If we are running, update the avg_rt tracking, as the running time
+	 * will now on be accounted into the latter.
+	 */
+	// /* 紀錄 應該用不到 */
+	// if (task_current(rq, p)) {
+	//	update_rt_rq_load_avg(rq_clock_pelt(rq), rq, 0); // q:不確定這邊要不要改
+	//	return;
+	// }
+	//
+	/*
+	 * If we are not running we may need to preempt the current
+	 * running task. If that current running task is also an RT task
+	 * then see if we can move to another run queue.
+	 */
+	if (task_on_rq_queued(p)) {
+		if (p->prio < rq->curr->prio && cpu_online(cpu_of(rq)))
+			resched_curr(rq);
+	}
+}
+
+
+static void prio_changed_mlq(struct rq *rq, struct task_struct *p, int oldprio)
+{
+	if (!task_on_rq_queued(p))
+		return;
+
+	if (task_current(rq, p)) {
+	/* For UP simply resched on drop of prio */
+		if (oldprio < p->prio)
+			resched_curr(rq);
+    /* CONFIG_SMP */
+	} else {
+		/*
+		 * This task is not running, but if it is
+		 * greater than the current running task
+		 * then reschedule.
+		 */
+		if (p->prio < rq->curr->prio)
+			resched_curr(rq);
+	}
+}
+
+/* Function to handle task tick for MLQ scheduler */
+static void task_tick_mlq(struct rq *rq, struct task_struct *p, int queued)
+{
+	// Your implementation to handle task tick for MLQ scheduler
+	struct sched_mlq_entity *mlq_se = &p->mlq;
+
+	update_curr_mlq(rq);
+	// update_rt_rq_load_avg(rq_clock_pelt(rq), rq, 1); *紀錄 應該用不到
+
+	// watchdog(rq, p); *紀錄 應該用不到
+
+	/*
+	 * RR tasks need a special form of timeslice management.
+	 * FIFO tasks have no timeslices.
+	 */
+	if (p->policy != SCHED_MLQ && p->prio > 141)
+		return;
+
+	if (--p->mlq.time_slice)
+		return;
+
+	if (p->prio == 140)
+		p->mlq.time_slice = TIME_SLICE_P1;
+	else
+		p->mlq.time_slice = TIME_SLICE_P2;
+	/*
+	 * Requeue to the end of queue if we (and all of our ancestors) are not
+	 * the only element on the queue
+	 */
+	for_each_sched_mlq_entity(mlq_se) {
+		if (mlq_se->run_list.prev != mlq_se->run_list.next) {
+			requeue_task_mlq(rq, p, 0); //注意此函式是否有實作
+			resched_curr(rq);		//(原本在yeild_task中有呼叫到)
+			return;
+		}
+	}
+}
+
+static unsigned int get_rr_interval_mlq(struct rq *rq, struct task_struct *p)
+{
+	/*
+	 * Time slice is 0 for SCHED_FIFO tasks
+	 */
+	if (p->prio == 140)
+		return TIME_SLICE_P1;
+	else if (p->prio == 141)
+		return TIME_SLICE_P2;
+	else
+		return 0;
+}
+
+
+//const struct sched_class mlq_sched_class
+DEFINE_SCHED_CLASS(mlq) = {
+
+	.enqueue_task		= enqueue_task_mlq,
+	.dequeue_task		= dequeue_task_mlq,
+	.yield_task		= yield_task_mlq,
+
+	.check_preempt_curr	= check_preempt_curr_mlq,
+
+	.pick_next_task		= pick_next_task_mlq,
+	.put_prev_task		= put_prev_task_mlq,
+	.set_next_task          = set_next_task_mlq,
+
+#ifdef CONFIG_SMP
+	.balance		= balance_rt,
+	.pick_task		= pick_task_rt,
+	.select_task_rq		= select_task_rq_rt,
+	.set_cpus_allowed       = set_cpus_allowed_common,
+	.rq_online              = rq_online_rt,
+	.rq_offline             = rq_offline_rt,
+	.task_woken		= task_woken_rt,
+	.switched_from		= switched_from_rt,
+	.find_lock_rq		= find_lock_lowest_rq,
+#endif
+
+	.task_tick		= task_tick_mlq,
+
+	.get_rr_interval	= get_rr_interval_mlq,
+
+	.prio_changed		= prio_changed_mlq,
+	.switched_to		= switched_to_mlq,
+
+	.update_curr		= update_curr_mlq,
+
+#ifdef CONFIG_UCLAMP_TASK
+	.uclamp_enabled		= 1,
+#endif
+};
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 3d3e5793e..5e1c48cbb 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -168,6 +168,11 @@ static inline int fair_policy(int policy)
 	return policy == SCHED_NORMAL || policy == SCHED_BATCH;
 }
 
+static inline int mlq_policy(int policy)
+{
+	return policy == SCHED_MLQ;
+}
+
 static inline int rt_policy(int policy)
 {
 	return policy == SCHED_FIFO || policy == SCHED_RR;
@@ -180,7 +185,7 @@ static inline int dl_policy(int policy)
 static inline bool valid_policy(int policy)
 {
 	return idle_policy(policy) || fair_policy(policy) ||
-		rt_policy(policy) || dl_policy(policy);
+		rt_policy(policy) || dl_policy(policy) || mlq_policy(policy);
 }
 
 static inline int task_has_idle_policy(struct task_struct *p)
@@ -193,6 +198,11 @@ static inline int task_has_rt_policy(struct task_struct *p)
 	return rt_policy(p->policy);
 }
 
+static inline int task_has_mlq_policy(struct task_struct *p)
+{
+	return mlq_policy(p->policy);
+}
+
 static inline int task_has_dl_policy(struct task_struct *p)
 {
 	return dl_policy(p->policy);
@@ -256,6 +266,11 @@ struct rt_prio_array {
 	struct list_head queue[MAX_RT_PRIO];
 };
 
+struct mlq_prio_array {
+	DECLARE_BITMAP(bitmap, MAX_MLQ_PRIO_USE+1); /* include 1 bit for delimiter */
+	struct list_head queue[MAX_MLQ_PRIO_USE];
+};
+
 struct rt_bandwidth {
 	/* nests inside the rq lock: */
 	raw_spinlock_t		rt_runtime_lock;
@@ -674,6 +689,43 @@ static inline bool rt_rq_is_runnable(struct rt_rq *rt_rq)
 	return rt_rq->rt_queued && rt_rq->rt_nr_running;
 }
 
+struct mlq_rq {
+	struct mlq_prio_array	active;
+	unsigned int		mlq_nr_running;
+	unsigned int		rr_nr_running;
+#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED
+	struct {
+		int		curr; /* highest queued rt task prio */
+#ifdef CONFIG_SMP
+		int		next; /* next highest */
+#endif
+	} highest_prio;
+#endif
+#ifdef CONFIG_SMP
+	unsigned int		rt_nr_migratory;
+	unsigned int		rt_nr_total;
+	int			overloaded;
+	struct plist_head	pushable_tasks;
+
+#endif /* CONFIG_SMP */
+	int			mlq_queued;
+
+	int			mlq_throttled;
+	u64			mlq_time;
+	u64			mlq_runtime;
+	/* Nests inside the rq lock: */
+	raw_spinlock_t		mlq_runtime_lock;
+
+#ifdef CONFIG_RT_GROUP_SCHED
+	unsigned int		mlq_nr_boosted;
+
+	struct rq		*rq;
+	struct task_group	*tg;
+#endif
+};
+
+
+
 /* Deadline class' related fields in a runqueue */
 struct dl_rq {
 	/* runqueue is an rbtree, ordered by deadline */
@@ -958,6 +1010,7 @@ struct rq {
 	struct cfs_rq		cfs;
 	struct rt_rq		rt;
 	struct dl_rq		dl;
+	struct mlq_rq       mlq;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this CPU: */
@@ -2217,6 +2270,7 @@ extern const struct sched_class dl_sched_class;
 extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
 extern const struct sched_class idle_sched_class;
+extern const struct sched_class mlq_sched_class;
 
 static inline bool sched_stop_runnable(struct rq *rq)
 {
@@ -2233,6 +2287,11 @@ static inline bool sched_rt_runnable(struct rq *rq)
 	return rq->rt.rt_queued > 0;
 }
 
+static inline bool sched_mlq_runnable(struct rq *rq)
+{
+	return rq->mlq.mlq_queued > 0;
+}
+
 static inline bool sched_fair_runnable(struct rq *rq)
 {
 	return rq->cfs.nr_running > 0;
@@ -2701,6 +2760,7 @@ static inline void resched_latency_warn(int cpu, u64 latency) {}
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq);
 extern void init_dl_rq(struct dl_rq *dl_rq);
+extern void init_mlq_rq(struct mlq_rq *mlq_rq);
 
 extern void cfs_bandwidth_usage_inc(void);
 extern void cfs_bandwidth_usage_dec(void);
